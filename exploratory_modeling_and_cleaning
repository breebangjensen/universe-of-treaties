#load packages
import pandas as pd
import re 
import gensim
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS
from nltk.stem import WordNetLemmatizer, SnowballStemmer
from nltk.stem.porter import *
import numpy as np
np.random.seed(2018)
import nltk
nltk.download('wordnet')

#set up dataframe
data = pd.read_csv('final_labels.csv', error_bad_lines=False);
data_text = data[['content']]
data_text['index'] = data_text.index
documents = data_text

#prelim text cleaning from Jose's script
#the text below doesn't work because documents isn't a string
#Do I need to feed it one string at a time (more loops?). Probably. 
#should we lemmetize next?)
#documents = re.findall('[A-Z][^A-Z]*', documents)
#documents = ' '.join(documents)
#documents = " ".join(w for w in nltk.wordpunct_tokenize(documents) if w.lower() in words or not w.isalpha())
#copying a different version from here: https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24
##doesn't quite work-- can return to later
#def lemmatize_stemming(text):
#    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))
#def preprocess(text):
#    result = []
#    for token in gensim.utils.simple_preprocess(text):
#       if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:
#            result.append(lemmatize_stemming(token))
#    return result

##check if it worked
#doc_sample = documents[documents['index'] == 543].values[0][0]
#print('original document: ')
#words = []
#for word in doc_sample.split(' '):
#    words.append(word)
#print(words)
#print('\n\n tokenized and lemmatized document: ')
#print(preprocess(doc_sample))

##process docs
#processed_docs = documents['headline_text'].map(preprocess)
#processed_docs[:10]

#ok let's pretend we processed it
##make dictionary based on term frequency
documents["documents"].to.list()
dictionary = gensim.corpora.Dictionary(documents)
count = 0
for k, v in dictionary.iteritems():
    print(k, v)
    count += 1
    if count > 10:
        break
